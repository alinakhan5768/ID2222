{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Locally Sensitive Hashing\n",
    "Locality Sensitive Hashing (LSH) is a technique used in computer science to solve the approximate or exact Near Neighbor Search in high-dimensional spaces. It is used to find similar items in a large dataset by hashing input items so that similar items map to the same \"buckets\" with high probability. LSH is commonly used in recommendation systems, image and audio recognition, and data mining.\n",
    "\n",
    "In this particular notebook we will implement a simplified version of the LSH algorithm for to compare texts and find how similar are. We will implement 4 classes which will help us to compute how similar 2 texts are. Those classes are: Shingling, CompareSets, MinHasing and CompareSignatures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "As a dataset, we have used the following texts:\n",
    " - Lorem Ipsum with 5 paragraphs (https://www.lipsum.com/)\\[1.txt\\]\n",
    " - Lorem Ipsum with 7 paragraphs (https://www.lipsum.com/)\\[2.txt\\]\n",
    " - Quijote de la Mancha by Miguel de Cervantes (https://www.gutenberg.org/cache/epub/60884/pg60884.txt)\\[3.txt\\]\n",
    " - The Adventures of Sherlock Holmes by Arthur Conan Doyle (https://www.gutenberg.org/cache/epub/1661/pg1661.txt)\\[4.txt\\]\n",
    " - The picture of Dorian Gray by Oscar Wilde (https://www.gutenberg.org/cache/epub/174/pg174.txt)\\[5.txt\\]\n",
    " - Beyond good and evil by Friedrich Wilheim Nietzsche (https://www.gutenberg.org/cache/epub/4363/pg4363.txt)\\[6.txt\\]\n",
    "\n",
    "This dataset has been selected like this, so it has two text which should be fairly similar (1 & 2), a text which should be fairly different (3) and three texts which should have something in common even though they may be different (4, 5 & 6).\n",
    "\n",
    "In this notebook we have implemented 4 classes which we will help us to compute the similarity of 2 texts. Those classes are: Shingling, CompareSets, MinHashing and CompareSignatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean venenatis, dui eget ullamcorper fermentum, odio nulla malesuada nulla, nec interdum lacus nisi a justo. Mauris ornare massa non nunc porttitor tristique. Duis tempor risus eget fermentum malesuada. Mauris sed neque quis risus venenatis sodales. Maecenas id elit posuere, lobortis mi nec, mollis mauris. Nam nulla ligula, ornare iaculis nulla non, pretium malesuada urna. Sed eros felis, porttitor vitae mollis vestibulum, consequat id mi. Donec at auctor dui, dignissim porta lorem.\n",
      "\n",
      "Quisque feugiat erat at ligula tincidunt, a pharetra lacus iaculis. Ut ac lobortis massa, vehicula hendrerit lectus. Cras molestie odio ac felis tincidunt mattis. Integer nec consequat odio, sit amet pellentesque neque. Praesent bibendum risus sollicitudin, consequat orci in, scelerisque erat. Quisque vestibulum arcu eget nulla porttitor, et elementum arcu venenatis. Ut vehicula nunc ac magna bibendum condimentum. Donec id sagittis tortor, hendrerit tempus felis. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed ultricies, ipsum vel efficitur venenatis, tortor felis dignissim arcu, a porta ex ipsum non orci. Sed et nisl aliquet, elementum felis a, vestibulum lorem. Etiam tempus neque arcu. Nunc est purus, maximus eget molestie at, tristique ut libero. Donec faucibus nisi urna, posuere imperdiet turpis hendrerit vitae. Nullam sodales lacus nec metus tincidunt, vel luctus enim laoreet.\n",
      "\n",
      "Duis facilisis hendrerit justo, quis interdum neque fermentum a. Maecenas ullamcorper magna lacus, in sagittis felis fringilla ut. Sed aliquam urna dictum, pretium tellus eget, aliquet tellus. Phasellus gravida tellus a erat mollis, sit amet pharetra erat consectetur. Phasellus commodo quam sit amet vulputate sodales. Quisque semper mauris vitae gravida iaculis. Suspendisse aliquam magna sit amet vestibulum congue. Ut a ex fringilla, vestibulum odio eu, finibus turpis. Quisque pretium viverra velit in vestibulum. Fusce tincidunt diam sit amet feugiat euismod. Integer laoreet lectus id neque ultrices, quis tristique felis condimentum. Sed tincidunt consectetur lorem, id iaculis lectus ultrices in. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Nulla felis eros, gravida sed quam a, pretium tincidunt sapien. Pellentesque molestie dui non consectetur vulputate.\n",
      "\n",
      "Sed vitae nisi fermentum, placerat ligula nec, pulvinar enim. Nam ullamcorper lacus eget nibh varius, id vehicula enim fermentum. Proin vehicula sit amet lectus a laoreet. Fusce viverra lectus eu tincidunt mattis. Pellentesque pharetra augue sit amet arcu varius accumsan. Donec egestas, tellus at sodales ornare, massa purus luctus odio, nec rhoncus mauris est eu tortor. Pellentesque volutpat ullamcorper ullamcorper. Donec at nunc pulvinar, bibendum magna non, ultrices arcu.\n",
      "\n",
      "Nulla sollicitudin ac sapien at dapibus. Sed at aliquam dolor, a iaculis sem. Donec non tincidunt urna. Aenean vitae porttitor orci. Maecenas semper tristique auctor. Vestibulum auctor ac nunc a vestibulum. Nullam nec egestas mi. Morbi dui nisl, elementum et pharetra nec, venenatis nec libero. Pellentesque eleifend ante ac augue rhoncus facilisis. Phasellus molestie, lorem vel molestie mollis, dolor mauris molestie risus, in gravida justo neque a dolor. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_path: str = \"dataset\"\n",
    "\n",
    "texts: list[str] = []\n",
    "\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(dataset_path, filename), \"r\") as f:\n",
    "            text = f.read()\n",
    "            texts.append(text)\n",
    "\n",
    "\n",
    "print(texts[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shingling\n",
    "Shingling is a technique used in text analysis to represent a document as a set of overlapping subsequences of fixed length k, called k-shingles. To compute shingles, we slide a window of size k over the document and extract the k-length substrings that fall within the window. We then store these substrings as a set, which represents the shingles of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shingling():\n",
    "    \n",
    "    def __init__(self, k: int):\n",
    "        self.k: int = k\n",
    "        \n",
    "    def get_shingles(self, document: str) -> set:\n",
    "        \"\"\"\n",
    "        This method constructs k-shingles of a given length k from a given document.\n",
    "        \"\"\"\n",
    "        shingles: set = set()\n",
    "        for i in range(len(document) - self.k + 1):\n",
    "            shingle: str = document[i:i+self.k]\n",
    "            shingles.add(shingle)\n",
    "        return shingles\n",
    "    \n",
    "    def get_hashed_shingles(self, document: str):\n",
    "        \"\"\"\n",
    "        This method computes a hash value for each unique shingle and represents the document in the form of an ordered set of its hashed k-shingles.\n",
    "        \"\"\"\n",
    "        shingles: set = self.get_shingles(document)\n",
    "        hashed_shingles: list[int] = [hash(shingle) for shingle in shingles]\n",
    "        hashed_shingles.sort()\n",
    "        return hashed_shingles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example to show what is shingling, we will use the following text:\n",
    "> \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "We can see that the text is divided on chunks of 3 characters like \"fox\", \"the\" or \"dog\". If there is a space in the middle it is also considered as a character. So, if we want to get the shingles of this text with a k=3, we will get the following shingles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' la', 'ick', 'er ', ' br', 'bro', 'jum', 'mps', 'n f', ' fo', 'uic', 'ump', 'ver', 'azy', 'r t', 'ove', 'ck ', 'wn ', ' qu', 'qui', 'x j', 'the', ' do', ' ov', 'laz', 'e l', ' th', 'zy ', 'k b', 'dog', 'row', 'fox', 'e q', 's o', 'own', 'The', 'ox ', ' ju', 'y d', 'he ', 'ps '}\n"
     ]
    }
   ],
   "source": [
    "example_text_1: str = \"The quick brown fox jumps over the lazy dog\"\n",
    "example_text_2: str = \"The agile black cat leaps over the active dog\"\n",
    "\n",
    "shingling: Shingling = Shingling(k=3)\n",
    "\n",
    "example_shingles_1: list[str] = shingling.get_shingles(example_text_1)\n",
    "print(example_shingles_1)\n",
    "\n",
    "example_hashed_shingles_texts: list[list[int]] = []\n",
    "for text in [example_text_1, example_text_2]:\n",
    "    example_hashed_shingles: int = shingling.get_hashed_shingles(text)\n",
    "    example_hashed_shingles_texts.append(example_hashed_shingles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do it with our text. We use a k=9 as we are analyzing text from books instead of emails. In the following lines we are doing exactly the same as we showed in the example above. In this case we will compute the hash of each shingle and we will store it in a set. We will do this for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "shingling: Shingling = Shingling(k=9)\n",
    "\n",
    "hashed_shingles_texts: list[list[int]] = []\n",
    "\n",
    "for text in texts:\n",
    "    hashed_shingles: int = shingling.get_hashed_shingles(text)\n",
    "    hashed_shingles_texts.append(hashed_shingles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinHashing\n",
    "### One-hot encoding\n",
    "Before computing the MinHashing we will use the **One-hot encoding** to represent the shingles of each text. This will help us to compute the Jaccard similarity. To create the one-hot encoding for each text we create a set with all the shingles of all the texts. Then, for each text we create a vector with the size of the set of all shingles. If the shingle is in the text, we will put a 1 in the vector, otherwise we will put a 0. This way we will have a vector for each text with the size of the set of all shingles. We will do this for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1], [0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "example_vocab_hashed: set = set()\n",
    "for hashed_shingles in example_hashed_shingles_texts:\n",
    "    for hashed_shingle in hashed_shingles:\n",
    "        example_vocab_hashed.add(hashed_shingle)\n",
    "\n",
    "example_matrix_one_hot_encoding: list[list[int]] = []\n",
    "for hashed_shingles in example_hashed_shingles_texts:\n",
    "    example_one_hot_encoding: list[int] = [1 if x in hashed_shingles else 0 for x in example_vocab_hashed]\n",
    "    example_matrix_one_hot_encoding.append(example_one_hot_encoding)\n",
    "print(example_matrix_one_hot_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code we do the same, but with the larger texts we originally had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_hashed: set = set()\n",
    "for hashed_shingles in hashed_shingles_texts:\n",
    "    for hashed_shingle in hashed_shingles:\n",
    "        vocab_hashed.add(hashed_shingle)\n",
    "\n",
    "matrix_one_hot_encoding: list[list[int]] = []\n",
    "for hashed_shingles in hashed_shingles_texts:\n",
    "    one_hot_encoding: list[int] = [1 if x in hashed_shingles else 0 for x in vocab_hashed]\n",
    "    matrix_one_hot_encoding.append(one_hot_encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHashing\n",
    "Now that we have the one-hot encoding for each text, we need to shuffle the texts multiple in order to create the signatures, which will be used to compute the similarity between the texts. We will do this by using the **MinHashing** technique. \n",
    "\n",
    "To compute the signature of each text we look at each col of the one-hot encoding. We need to find the first row which has a 1. We will store the row number in the signature. We will do this for each col of the one-hot encoding. This way we will have a signature for each text. We will do this for each text.\n",
    "\n",
    "We will use our previous example to show how MinHashing works. First of all, we create a list with all the indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]\n"
     ]
    }
   ],
   "source": [
    "example_hash_indexes: list[int] = list(range(1, len(example_vocab_hashed)+1))\n",
    "print(example_hash_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have the list, we shuffle it one time (in this case). We will use this shuffled list to compute the signature of each text. We will do this for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> 63\n",
      "2 -> 36\n",
      "3 -> 17\n",
      "4 -> 29\n",
      "5 -> 57\n",
      "6 -> 51\n",
      "7 -> 30\n",
      "8 -> 33\n",
      "9 -> 11\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "shuffle(example_hash_indexes)\n",
    "\n",
    "for i in range(1, 10):\n",
    "    print(f\"{i} -> {example_hash_indexes.index(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class MinHashing is used to compute all the different shuffle needed to create the signature. It can be thought as an auxiliary class which assists Signature. Basically, it does what we have shown previously. In this case we can set as well how many shuffles we want to do. One shuffle is equivalent to one bit in the signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinHashing():\n",
    "\n",
    "    def __init__(self, vocab_size: int, nbits: int):\n",
    "        self.vocab_size: int = vocab_size\n",
    "        self.nbits: int = nbits\n",
    "        self.hashes: list[int] = []\n",
    "\n",
    "    def create_hash_functions(self) -> list[int]:\n",
    "        \"\"\"\n",
    "        Function for creating the hash vector / function\n",
    "        \"\"\"\n",
    "        hash_indexes: list[int] = list(range(1, self.vocab_size+1))\n",
    "        shuffle(hash_indexes)\n",
    "        return hash_indexes\n",
    "    \n",
    "    def build_minhashing_functions(self) -> list[int]:\n",
    "        \"\"\"\n",
    "        Function for building multiple minhashing vectors\n",
    "        \"\"\"\n",
    "        hashes: list[int] = []\n",
    "        for _ in range(self.nbits):\n",
    "            hashes.append(\n",
    "                self.create_hash_functions()\n",
    "            )\n",
    "        return hashes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signature\n",
    "To illustrate how the creation of the signature works, we will illustrate first how the signature finds the first bit. In this case it looks for the first index with a 1 in the column. This index will be the first bit of the signature. We will do this for each column of the one-hot encoding. This way we will have a signature for each text. We will do this for each text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Index: 63 -> 0\n",
      "2. Index: 36 -> 0\n",
      "3. Index: 17 -> 1\n",
      "Match!\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(example_vocab_hashed)+1):\n",
    "    idx: int = example_hash_indexes.index(i)\n",
    "    signature_value: int = matrix_one_hot_encoding[0][idx]\n",
    "    print(f\"{i}. Index: {idx} -> {signature_value}\")\n",
    "    if signature_value == 1:\n",
    "        print(\"Match!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, after a several indexes, we find a 1. When we have the match, we store the index in the signature. We will do this for each text. For example, if the signature is of 20 bits, we will do this 20 times.\n",
    "\n",
    "To compute this in a more systematic way, we have the class Signature which does all this computations. Internally, it computes the minhashing to create the different shuffled vectors for the indexes and then it computes the signature. We will do this for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Signature():\n",
    "    \n",
    "    def __init__(self, matrix_one_hot_encoding: list[list[int]], nbits: int = 20):\n",
    "        self.vocab_size: int = len(matrix_one_hot_encoding[0])\n",
    "        minhashing = MinHashing(self.vocab_size, nbits)\n",
    "        self.minhash_functions: list[list[int]] = minhashing.build_minhashing_functions()\n",
    "\n",
    "    def create_hash(self, vector: list[int]) -> list[int]:\n",
    "        \"\"\"\n",
    "        This function creates our signatures matching the 1s\n",
    "        \"\"\"\n",
    "        signature: list[int] = []\n",
    "        indices_of_ones = {i: 1 for i, v in enumerate(vector) if v == 1}\n",
    "        for function in self.minhash_functions:\n",
    "            for i in range(1, self.vocab_size+1):\n",
    "                idx: int = function.index(i)\n",
    "                if idx in indices_of_ones:\n",
    "                    signature.append(idx)\n",
    "                    break\n",
    "        return signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will look which are the signatures with the examples texts. Later, we will use this signatures to compute the similarity between the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 39, 29, 31, 32, 13, 33, 0, 11, 15, 11, 11, 7, 12, 9, 9, 41, 6, 11, 33]\n",
      "[38, 23, 34, 31, 32, 4, 19, 28, 20, 36, 36, 38, 25, 23, 40, 37, 41, 6, 22, 33]\n"
     ]
    }
   ],
   "source": [
    "example_texts_signatures: Signature = Signature([example_text_1, example_text_2])\n",
    "\n",
    "example_text_1_signature: list[int] = example_texts_signatures.create_hash(example_matrix_one_hot_encoding[0])\n",
    "print(example_text_1_signature)\n",
    "\n",
    "example_text_2_signature: list[int] = example_texts_signatures.create_hash(example_matrix_one_hot_encoding[1])\n",
    "print(example_text_2_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this examples, we can see that some values in the signature are similar. This means that the texts are similar at least in some part. This makes sense as if we look at the examples text we can see that they have some words in common.\n",
    "\n",
    "- Example text 1: \"The quick brown fox jumps over the lazy dog\"\n",
    "- Example text 2: \"The agile black cat leaps over the active dog\"\n",
    "\n",
    "In this example, we can expect that there is some similarity, as there are some similar words like \"dog\", \"over\" or \"the\". However, we can see that the similarity is not that high. This is because the texts are not that similar.\n",
    "\n",
    "Now we do it, with our dataset, instead of the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_signatures_func: Signature = Signature(matrix_one_hot_encoding, nbits=1000)\n",
    "\n",
    "texts_signatures: list[list[int]] = []\n",
    "for text_one_hot_encoding in matrix_one_hot_encoding:\n",
    "    text_signature: list[int] = texts_signatures_func.create_hash(text_one_hot_encoding)\n",
    "    texts_signatures.append(text_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the comparison of the signatures manually if we want. Nonetheless, looking at this manually for each text would be really difficult. This is where the Jaccard similarity can help to compute how similar texts are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CompareSignatures\n",
    "To compare the signatures and see how close those texts are we can compute the jaccard similarity. The jaccard similarity is computed by looking at the number of elements in common between two sets and dividing it by the total number of elements in both sets. In our case, we will look at the number of elements in common between two signatures and dividing it by the total number of elements in both signatures. This way we will get a value between 0 and 1. The closer the value is to 1, the more similar the texts are. The closer the value is to 0, the less similar the texts are.\n",
    "\n",
    "For the jaccard similarity we will use the following formula:\n",
    "\n",
    "$$J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "In the following lines we will compute the Jaccard similarity of each pair of texts in the example texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.19230769230769232, 0.6428571428571429)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard(x: set, y: set):\n",
    "    return len(x.intersection(y)) / len(x.union(y))\n",
    "\n",
    "jaccard(set(example_text_1_signature), set(example_text_2_signature)), jaccard(set(example_text_1), set(example_text_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the example texts:\n",
    "- Example text 1: \"The quick brown fox jumps over the lazy dog\"\n",
    "- Example text 2: \"The agile black cat leaps over the active dog\"\n",
    "\n",
    "In this particular case, the result is quite interesting. If we look at the similarity without doing the minhashing we can observe that the texts are somehow similar. However, if we look at the similarity after doing the minhashing we can see that the similarity is not that high. Both text they have repeated words like \"The\", \"over\" or \"dog\" and quite similar length, however everything else is not exactly the same. This is why the similarity is not that high. This is a good example of how the minhashing can help us to compute the similarity between texts.\n",
    "\n",
    "We will create a class and a function to compute the jaccard similarity in order to be more clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(x: set, y: set) -> float:\n",
    "    \"\"\"\n",
    "    This method computes the Jaccard similarity of two sets.\n",
    "    \"\"\"\n",
    "    intersection_size: int = len(x.intersection(y))\n",
    "    union_size: int = len(x.union(y))\n",
    "    jaccard_similarity: float = intersection_size / union_size\n",
    "    return jaccard_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our dataset and computing the jaccard similarity for all the texts with the minhashing and in the original form we can see the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_jaccard_similarities: list[list[float]] = []\n",
    "signatures_jaccard_similarities: list[list[float]] = []\n",
    "\n",
    "for text_x in texts:\n",
    "    text_jaccard_similarities_with_other_texts: list[int] = []\n",
    "    for text_y in texts:\n",
    "        text_jaccard_similarities_with_other_texts.append(\n",
    "            jaccard_similarity(set(text_x), set(text_y))\n",
    "        )\n",
    "    sets_jaccard_similarities.append(text_jaccard_similarities_with_other_texts)\n",
    "\n",
    "for text_signature_x in texts_signatures:\n",
    "    text_signature_jaccard_similarities_with_other_texts: list[int] = []\n",
    "    for text_signature_y in texts_signatures:\n",
    "        text_signature_jaccard_similarities_with_other_texts.append(\n",
    "            jaccard_similarity(set(text_signature_x), set(text_signature_y))\n",
    "        )\n",
    "    signatures_jaccard_similarities.append(text_signature_jaccard_similarities_with_other_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the similarities pair we can spot some interesting things:\n",
    "- Text 1 and 2 are quite similar. This makes sense as they are the same text but with different number of paragraphs.\n",
    "- The rest of the texts are not that similar. This makes sense as they are different texts.\n",
    "\n",
    "It is good to notice that the each row represent the text in the position of the row. For example, the first row represents the similarity of the text 1 with the rest of the texts. The same with the columns. This is why the matrix is mirrored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarities for direct sets\n",
      "\n",
      "[[1.    0.717 0.761 0.647 0.567 0.583]\n",
      " [0.717 1.    0.687 0.646 0.679 0.667]\n",
      " [0.761 0.687 1.    0.718 0.6   0.615]\n",
      " [0.647 0.646 0.718 1.    0.69  0.707]\n",
      " [0.567 0.679 0.6   0.69  1.    0.976]\n",
      " [0.583 0.667 0.615 0.707 0.976 1.   ]]\n",
      "\n",
      "--------------------\n",
      "\n",
      "Jaccard Similarities for signatures\n",
      "\n",
      "[[1.    0.004 0.002 0.    0.    0.   ]\n",
      " [0.004 1.    0.004 0.    0.    0.   ]\n",
      " [0.002 0.004 1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.509]\n",
      " [0.    0.    0.    0.    0.509 1.   ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Jaccard Similarities for direct sets\")\n",
    "print(\"\")\n",
    "print(np.round(np.matrix(sets_jaccard_similarities), 3))\n",
    "print(\"\")\n",
    "print(\"-\"*20)\n",
    "print(\"\")\n",
    "print(\"Jaccard Similarities for signatures\")\n",
    "print(\"\")\n",
    "print(np.round(np.matrix(signatures_jaccard_similarities), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH: Locality Sensitive Hashing\n",
    "Now we are going to implement the LSH algorithm. This algorithm will help us to find the similarity between texts if we have bigger texts. For now, we have used really small text samples, but if we have bigger texts, the minhashing will take a lot of time to compute. This is why we will use the LSH algorithm. This algorithm will help us to find the similarity between texts without having to compute the minhashing for all the texts. This way we will save a lot of time.\n",
    "\n",
    "The first thing we need to do is to create the bands. The bands are used to create the buckets. The buckets are used to store the signatures. The signatures are used to compute the similarity between texts. We will do this for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH():\n",
    "    def split_vector(self, signature: list[int], b: int):\n",
    "        \"\"\"\n",
    "        This function splits the signature in b parts.\n",
    "        In case the signature it can not be splitted in equally b parts,\n",
    "        it throws an error. \n",
    "        \"\"\"\n",
    "        assert len(signature) % b == 0\n",
    "        r: int = int(len(signature) / b)\n",
    "        subvectors: list[list[int]] = []\n",
    "        for i in range(0, len(signature), r):\n",
    "            subvectors.append(signature[i : i+r])\n",
    "        return subvectors\n",
    "    \n",
    "    def find_matches(self, band_1: list[int], band_2: list[int]):\n",
    "        \"\"\"\n",
    "        This function looks for candidate pairs that matches in both bands. \n",
    "        \"\"\"\n",
    "        for rows_1, rows_2 in zip(band_1, band_2):\n",
    "            if rows_1 == rows_2:\n",
    "                print(f\"Candidate pair found: {rows_1} == {rows_2}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LSH.find_matches() missing 1 required positional argument: 'band_2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/enric/Developer/ID2222/hw1/homework1.ipynb Cell 39\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/enric/Developer/ID2222/hw1/homework1.ipynb#Y103sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m example_band_text_1 \u001b[39m=\u001b[39m lsh\u001b[39m.\u001b[39msplit_vector(example_text_1_signature, \u001b[39m10\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/enric/Developer/ID2222/hw1/homework1.ipynb#Y103sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m example_band_text_2 \u001b[39m=\u001b[39m lsh\u001b[39m.\u001b[39msplit_vector(example_text_2_signature, \u001b[39m10\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/enric/Developer/ID2222/hw1/homework1.ipynb#Y103sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m LSH\u001b[39m.\u001b[39mfind_matches(example_band_text_1, example_band_text_2)\n",
      "\u001b[0;31mTypeError\u001b[0m: LSH.find_matches() missing 1 required positional argument: 'band_2'"
     ]
    }
   ],
   "source": [
    "lsh: LSH = LSH()\n",
    "example_band_text_1 = lsh.split_vector(example_text_1_signature, 10)\n",
    "example_band_text_2 = lsh.split_vector(example_text_2_signature, 10)\n",
    "lsh.find_matches(example_band_text_1, example_band_text_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
